+++
title = "Veille technologique"
weight = 1
+++

Question envoyÃ©e Ã  lâ€™intelligence artificielle : 
```
Explique-moi ce que sont les LLMs locaux, Ollama et le RAG.
```
RÃ©ponse :
```
Voici une explication claire et complÃ¨te â€” simple, mais assez approfondie pour bien comprendre LLM locaux, Ollama, et le RAG, mÃªme si tâ€™es nouveau dans ce monde-lÃ .
________________________________________
âœ… 1. Câ€™est quoi un LLM local ?
Un LLM local (Large Language Model local) est un modÃ¨le dâ€™intelligence artificielle qui tourne directement sur ton ordinateur, sans envoyer tes donnÃ©es sur Internet.
ğŸ”¥ Avantages :
â€¢	ConfidentialitÃ© totale : tout reste sur ton PC.
â€¢	Aucun abonnement : une fois installÃ©, câ€™est gratuit.
â€¢	Ultra rapide si ton PC est puissant (GPU, RAM).
â— InconvÃ©nients :
â€¢	Peut demander beaucoup de puissance (VRAM, RAM).
â€¢	Les modÃ¨les locaux sont parfois moins puissants que les modÃ¨les en ligne comme GPT-5 ou Claude 3.7.
ğŸ“Œ Exemples de LLM locaux :
â€¢	Llama 3
â€¢	Qwen2
â€¢	Mistral 7B / Mixtral
â€¢	Phi-3
Ce sont des modÃ¨les que tu peux tÃ©lÃ©charger (comme un fichier .gguf) et faire tourner chez toi.
________________________________________
âœ… 2. Câ€™est quoi Ollama ?
Ollama, câ€™est une application super simple qui permet :
ğŸ‘‰ dâ€™installer,
ğŸ‘‰ de gÃ©rer,
ğŸ‘‰ et de faire tourner des LLM locaux sur ton PC (Windows, Mac, Linux)
â¤ Comment Ã§a fonctionne ?
Tu installes Ollama â†’ puis tu peux tÃ©lÃ©charger un modÃ¨le avec une simple commande comme :
ollama run llama3
Et Ã§a marche directement :
tu lui poses des questions, il rÃ©pond, comme ChatGPT mais en local.
ğŸ‘ Points forts :
â€¢	TrÃ¨s facile Ã  utiliser.
â€¢	TÃ©lÃ©charge automatique les modÃ¨les optimisÃ©s.
â€¢	Compatible avec plein dâ€™outils (LM Studio, OpenWebUI, VSCode, etc.).
â€¢	Tu peux crÃ©er tes propres modÃ¨les (modelfile).
ğŸŒ Ollama devient la â€œrÃ©fÃ©renceâ€ actuelle pour les LLM locaux.
________________________________________
âœ… 3. Câ€™est quoi le RAG ? (Retrieval Augmented Generation)
Le RAG, câ€™est une technique qui permet Ã  un LLM de lire tes documents avant de rÃ©pondre.
ğŸ§  En gros :
1.	Tu donnes des documents (PDF, sites web, notes, Word, etc.).
2.	Un moteur va chercher (retrieval) lâ€™information pertinente.
3.	Le LLM utilise seulement ces infos pour gÃ©nÃ©rer (generation) la rÃ©ponse.
ğŸ”¥ Pourquoi câ€™est puissant ?
Parce que Ã§a permet Ã  un modÃ¨le moins intelligent (ex : Llama 3 8B) de rÃ©pondre Ã  des questions de maniÃ¨re aussi prÃ©cise quâ€™un grand modÃ¨le, tant qu'il a accÃ¨s aux bons documents.
ğŸ“Œ Exemple concrÃ¨t :
Tu donnes tes documents de budgets, rÃ¨glements, emails, etc.
â†’ Le LLM peut te faire :
â€¢	RÃ©sumÃ©s
â€¢	Analyses
â€¢	Rapports
â€¢	Ã‰bauches de mails
â€¢	RÃ©Ã©criture de documents
â€¢	Conseils adaptÃ©s Ã  TON organisation
Câ€™est comme avoir ton propre ChatGPT entraÃ®nÃ© sur tes documents â€” mais sans lâ€™entraÃ®ner pour vrai.
________________________________________
ğŸ¯ RÃ©sumÃ© en une ligne
â€¢	LLM local : un gros modÃ¨le IA qui roule sur ton PC.
â€¢	Ollama : le logiciel le plus simple pour faire tourner ces modÃ¨les.
â€¢	RAG : une technique qui permet Ã  ton IA dâ€™utiliser tes documents pour rÃ©pondre prÃ©cisÃ©ment.
```

# Analyse de la rÃ©ponse fournie

AprÃ¨s avoir lu la rÃ©ponse de Chatgpt 5.1 Instant. Nous pouvons constater que la rÃ©ponse est bien formulÃ©e en globalitÃ©, le LLM utilise des mots simples et faciles Ã  comprendre, il explique en assez de mots les diffÃ©rentes technologies pour que lâ€™on puisse comprendre la base de ce que câ€™est un LLM local, Ollama ainsi que le RAG. 

Cependant, je dirais que la rÃ©ponse reste assez vague, elle explique la base des technologies, mais elle ne va pas beaucoup plus loin que Ã§a dans la technique.

Par exemple, la rÃ©ponse ne parle pas des limites du RAG, comme des potentielles hallucinations persistantes, câ€™est-Ã -dire que le LLM invente des fausses informations, mÃªme si ce problÃ¨me essaie dâ€™Ãªtre de plus en plus corrigÃ© avec les nouveaux modÃ¨les. De plus vu que les LLMS ne pensent pas et quâ€™ils prÃ©disent statistiquement les mots, Ã§a peut entraÃ®ner de mauvaises interprÃ©tations.

De plus, en regardant sur diffÃ©rentes pages web, on peut facilement trouver les mÃªmes informations et mÃªme de faÃ§ons plus dÃ©veloppÃ©es. Par exemple, sur la page WikipÃ©dia Â« Retrieval-Augmented Generation Â», on explique le fonctionnement interne du RAG : la page parle des embeddings (vecteurs numÃ©riques) qui transforment les mots/paragraphes en suite de nombres. Ce qui facilite lâ€™intelligence artificielle Ã  comprendre. On parle aussi des retrievers qui servent Ã  retrouver les documents les plus pertinents en fonction de la question qui lui a Ã©tÃ© posÃ©e ainsi que dâ€™autres mÃ©canismes internes utilisÃ©s par le RAG.

Les diffÃ©rentes sources fournissent des informations beaucoup plus dÃ©veloppÃ©es sur les LLM que la rÃ©ponse reÃ§ue par ChatGpt. Dans lâ€™article : Â« What are LLMs, Local LLMs and RAG? Â» On nous apprend comment construire et entraÃ®ner un modÃ¨le : en lâ€™entraÃ®nant comme on lâ€™a aussi vu en cours (machine learning), ou bien avec de la collecte de donnÃ©es.  

En comparaison Ã  un moteur de recherche comme Google, la rÃ©ponse de ChatGpt est trÃ¨s facile Ã  lire et Ã  comprendre, comme je lâ€™ai dit un peu plus haut. La rÃ©ponse est plus ou moins la mÃªme si on pose la question Ã  Gemini. Pour une personne qui nâ€™en saurait rien sur le sujet et qui aimerait simplement faire une courte recherche sur le sujet, je pense que cela suffirait. Mais si on veut sâ€™approfondir un peu plus sur le sujet, avec une recherche Google nous pourrions directement tomber sur diffÃ©rents sites web ou vidÃ©os youtubes qui fourniraient des rÃ©ponses plus dÃ©veloppÃ©es et plus techniques.

Pour conclure, il nâ€™y a pas un site exactement qui possÃ¨de toutes les informations concernant les LLM, Ollama ainsi que le RAG, mais en faisant quelques petites recherches, nous pouvons rapidement trouver des informations intÃ©ressantes et plus poussÃ©es sur le sujet que la simple recherche sur ChatGpt ou bien Gemini. Il y a aussi des vidÃ©os complÃ¨tes qui expliquent le sujet de maniÃ¨re assez claire.

## Sources : 

**Comment les LLM marchent** 
- https://www.youtube.com/watch?v=5sLYAQS9sWQ&t=2s

**Explication dÃ©taillÃ©e de llama3.1**
- https://ai.meta.com/blog/meta-llama-3-1/ 

**Explication du RAG**
- https://en.wikipedia.org/wiki/Retrieval-augmented_generation 

**Explication LLM, Local LLM et RAG**
- https://medium.com/@arjunrao87/what-are-llms-local-llms-and-rag-0198868f8657

##### VidÃ©os youtubes : 

**Explication du fonctionnement des LLM**
- https://www.youtube.com/watch?v=5sLYAQS9sWQ&t=2s 

**Câ€™est quoi un RAG**
- https://www.youtube.com/watch?v=T-D1OfcDW1M 

**Câ€™est quoi llama3.1**
- https://youtu.be/8c2LnKNoSmg